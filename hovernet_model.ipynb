{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1818e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_op(x, cropping, data_format=\"NCHW\"):\n",
    "    \"\"\"Center crop image.\n",
    "\n",
    "    Args:\n",
    "        x: input image\n",
    "        cropping: the substracted amount\n",
    "        data_format: choose either `NCHW` or `NHWC`\n",
    "        \n",
    "    \"\"\"\n",
    "    crop_t = cropping[0] // 2\n",
    "    crop_b = cropping[0] - crop_t\n",
    "    crop_l = cropping[1] // 2\n",
    "    crop_r = cropping[1] - crop_l\n",
    "    if data_format == \"NCHW\":\n",
    "        x = x[:, :, crop_t:-crop_b, crop_l:-crop_r]\n",
    "    else:\n",
    "        x = x[:, crop_t:-crop_b, crop_l:-crop_r, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "####\n",
    "def crop_to_shape(x, y, data_format=\"NCHW\"):\n",
    "    \"\"\"Centre crop x so that x has shape of y. y dims must be smaller than x dims.\n",
    "\n",
    "    Args:\n",
    "        x: input array\n",
    "        y: array with desired shape.\n",
    "\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        y.shape[0] <= x.shape[0] and y.shape[1] <= x.shape[1]\n",
    "    ), \"Ensure that y dimensions are smaller than x dimensions!\"\n",
    "\n",
    "    x_shape = x.size()\n",
    "    y_shape = y.size()\n",
    "    if data_format == \"NCHW\":\n",
    "        crop_shape = (x_shape[2] - y_shape[2], x_shape[3] - y_shape[3])\n",
    "    else:\n",
    "        crop_shape = (x_shape[1] - y_shape[1], x_shape[2] - y_shape[2])\n",
    "    return crop_op(x, crop_shape, data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b3da8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\" A base class provides a common weight initialisation scheme.\"\"\"\n",
    "\n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            classname = m.__class__.__name__\n",
    "\n",
    "            # ! Fixed the type checking\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "            if \"norm\" in classname.lower():\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            if \"linear\" in classname.lower():\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class TFSamepaddingLayer(nn.Module):\n",
    "    \"\"\"To align with tf `same` padding. \n",
    "    \n",
    "    Putting this before any conv layer that need padding\n",
    "    Assuming kernel has Height == Width for simplicity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ksize, stride):\n",
    "        super(TFSamepaddingLayer, self).__init__()\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[2] % self.stride == 0:\n",
    "            pad = max(self.ksize - self.stride, 0)\n",
    "        else:\n",
    "            pad = max(self.ksize - (x.shape[2] % self.stride), 0)\n",
    "\n",
    "        if pad % 2 == 0:\n",
    "            pad_val = pad // 2\n",
    "            padding = (pad_val, pad_val, pad_val, pad_val)\n",
    "        else:\n",
    "            pad_val_start = pad // 2\n",
    "            pad_val_end = pad - pad_val_start\n",
    "            padding = (pad_val_start, pad_val_end, pad_val_start, pad_val_end)\n",
    "        x = F.pad(x, padding, \"constant\", 0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseBlock(Net):\n",
    "    \"\"\"Dense Block as defined in:\n",
    "\n",
    "    Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. \n",
    "    \"Densely connected convolutional networks.\" In Proceedings of the IEEE conference \n",
    "    on computer vision and pattern recognition, pp. 4700-4708. 2017.\n",
    "\n",
    "    Only performs `valid` convolution.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, unit_ksize, unit_ch, unit_count, split=1):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        assert len(unit_ksize) == len(unit_ch), \"Unbalance Unit Info\"\n",
    "\n",
    "        self.nr_unit = unit_count\n",
    "        self.in_ch = in_ch\n",
    "        self.unit_ch = unit_ch\n",
    "\n",
    "        # ! For inference only so init values for batchnorm may not match tensorflow\n",
    "        unit_in_ch = in_ch\n",
    "        self.units = nn.ModuleList()\n",
    "        for idx in range(unit_count):\n",
    "            self.units.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"preact_bna/bn\", nn.BatchNorm2d(unit_in_ch, eps=1e-5)),\n",
    "                            (\"preact_bna/relu\", nn.ReLU(inplace=True)),\n",
    "                            (\n",
    "                                \"conv1\",\n",
    "                                nn.Conv2d(\n",
    "                                    unit_in_ch,\n",
    "                                    unit_ch[0],\n",
    "                                    unit_ksize[0],\n",
    "                                    stride=1,\n",
    "                                    padding=0,\n",
    "                                    bias=False,\n",
    "                                ),\n",
    "                            ),\n",
    "                            (\"conv1/bn\", nn.BatchNorm2d(unit_ch[0], eps=1e-5)),\n",
    "                            (\"conv1/relu\", nn.ReLU(inplace=True)),\n",
    "                            # ('conv2/pool', TFSamepaddingLayer(ksize=unit_ksize[1], stride=1)),\n",
    "                            (\n",
    "                                \"conv2\",\n",
    "                                nn.Conv2d(\n",
    "                                    unit_ch[0],\n",
    "                                    unit_ch[1],\n",
    "                                    unit_ksize[1],\n",
    "                                    groups=split,\n",
    "                                    stride=1,\n",
    "                                    padding=0,\n",
    "                                    bias=False,\n",
    "                                ),\n",
    "                            ),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            unit_in_ch += unit_ch[1]\n",
    "\n",
    "        self.blk_bna = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"bn\", nn.BatchNorm2d(unit_in_ch, eps=1e-5)),\n",
    "                    (\"relu\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def out_ch(self):\n",
    "        return self.in_ch + self.nr_unit * self.unit_ch[-1]\n",
    "\n",
    "    def forward(self, prev_feat):\n",
    "        for idx in range(self.nr_unit):\n",
    "            new_feat = self.units[idx](prev_feat)\n",
    "            prev_feat = crop_to_shape(prev_feat, new_feat)\n",
    "            prev_feat = torch.cat([prev_feat, new_feat], dim=1)\n",
    "        prev_feat = self.blk_bna(prev_feat)\n",
    "        return prev_feat\n",
    "\n",
    "\n",
    "class ResidualBlock(Net):\n",
    "    \"\"\"Residual block as defined in:\n",
    "\n",
    "    He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep residual learning \n",
    "    for image recognition.\" In Proceedings of the IEEE conference on computer vision \n",
    "    and pattern recognition, pp. 770-778. 2016.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, unit_ksize, unit_ch, unit_count, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        assert len(unit_ksize) == len(unit_ch), \"Unbalance Unit Info\"\n",
    "\n",
    "        self.nr_unit = unit_count\n",
    "        self.in_ch = in_ch\n",
    "        self.unit_ch = unit_ch\n",
    "\n",
    "        # ! For inference only so init values for batchnorm may not match tensorflow\n",
    "        unit_in_ch = in_ch\n",
    "        self.units = nn.ModuleList()\n",
    "        for idx in range(unit_count):\n",
    "            unit_layer = [\n",
    "                (\"preact/bn\", nn.BatchNorm2d(unit_in_ch, eps=1e-5)),\n",
    "                (\"preact/relu\", nn.ReLU(inplace=True)),\n",
    "                (\n",
    "                    \"conv1\",\n",
    "                    nn.Conv2d(\n",
    "                        unit_in_ch,\n",
    "                        unit_ch[0],\n",
    "                        unit_ksize[0],\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "                (\"conv1/bn\", nn.BatchNorm2d(unit_ch[0], eps=1e-5)),\n",
    "                (\"conv1/relu\", nn.ReLU(inplace=True)),\n",
    "                (\n",
    "                    \"conv2/pad\",\n",
    "                    TFSamepaddingLayer(\n",
    "                        ksize=unit_ksize[1], stride=stride if idx == 0 else 1\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    \"conv2\",\n",
    "                    nn.Conv2d(\n",
    "                        unit_ch[0],\n",
    "                        unit_ch[1],\n",
    "                        unit_ksize[1],\n",
    "                        stride=stride if idx == 0 else 1,\n",
    "                        padding=0,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "                (\"conv2/bn\", nn.BatchNorm2d(unit_ch[1], eps=1e-5)),\n",
    "                (\"conv2/relu\", nn.ReLU(inplace=True)),\n",
    "                (\n",
    "                    \"conv3\",\n",
    "                    nn.Conv2d(\n",
    "                        unit_ch[1],\n",
    "                        unit_ch[2],\n",
    "                        unit_ksize[2],\n",
    "                        stride=1,\n",
    "                        padding=0,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "            # * has bna to conclude each previous block so\n",
    "            # * must not put preact for the first unit of this block\n",
    "            unit_layer = unit_layer if idx != 0 else unit_layer[2:]\n",
    "            self.units.append(nn.Sequential(OrderedDict(unit_layer)))\n",
    "            unit_in_ch = unit_ch[-1]\n",
    "\n",
    "        if in_ch != unit_ch[-1] or stride != 1:\n",
    "            self.shortcut = nn.Conv2d(in_ch, unit_ch[-1], 1, stride=stride, bias=False)\n",
    "        else:\n",
    "            self.shortcut = None\n",
    "\n",
    "        self.blk_bna = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"bn\", nn.BatchNorm2d(unit_in_ch, eps=1e-5)),\n",
    "                    (\"relu\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def out_ch(self):\n",
    "        return self.unit_ch[-1]\n",
    "\n",
    "    def forward(self, prev_feat, freeze=False):\n",
    "        if self.shortcut is None:\n",
    "            shortcut = prev_feat\n",
    "        else:\n",
    "            shortcut = self.shortcut(prev_feat)\n",
    "\n",
    "        for idx in range(0, len(self.units)):\n",
    "            new_feat = prev_feat\n",
    "            if self.training:\n",
    "                with torch.set_grad_enabled(not freeze):\n",
    "                    new_feat = self.units[idx](new_feat)\n",
    "            else:\n",
    "                new_feat = self.units[idx](new_feat)\n",
    "            prev_feat = new_feat + shortcut\n",
    "            shortcut = prev_feat\n",
    "        feat = self.blk_bna(prev_feat)\n",
    "        return feat\n",
    "\n",
    "\n",
    "class UpSample2x(nn.Module):\n",
    "    \"\"\"Upsample input by a factor of 2.\n",
    "    \n",
    "    Assume input is of NCHW, port FixedUnpooling from TensorPack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UpSample2x, self).__init__()\n",
    "        # correct way to create constant within module\n",
    "        self.register_buffer(\"unpool_mat\", torch.ones((2, 2), dtype=torch.float32))\n",
    "        self.unpool_mat.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_shape = list(x.shape)\n",
    "        # unsqueeze is expand_dims equivalent\n",
    "        # permute is transpose equivalent\n",
    "        # view is reshape equivalent\n",
    "        x = x.unsqueeze(-1)  # bchwx1\n",
    "        mat = self.unpool_mat.unsqueeze(0)  # 1xshxsw\n",
    "        ret = torch.tensordot(x, mat, dims=1)  # bxcxhxwxshxsw\n",
    "        ret = ret.permute(0, 1, 2, 4, 3, 5)\n",
    "        ret = ret.reshape((-1, input_shape[1], input_shape[2] * 2, input_shape[3] * 2))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "49a1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoVerNet(Net):\n",
    "    \"\"\"Initialise HoVer-Net.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ch=3, nr_types=None, freeze=False, mode='original'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.freeze = freeze\n",
    "        self.nr_types = nr_types\n",
    "        self.output_ch = 3 if nr_types is None else 4\n",
    "\n",
    "        assert mode == 'original' or mode == 'fast', 'Unknown mode `%s` for HoVerNet %s. Only support `original` or `fast`.' % mode\n",
    "\n",
    "        module_list = [\n",
    "            (\"/\", nn.Conv2d(input_ch, 64, 7, stride=1, padding=0, bias=False)),\n",
    "            (\"bn\", nn.BatchNorm2d(64, eps=1e-5)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "        ]\n",
    "        if mode == 'fast': # prepend the padding for `fast` mode\n",
    "            module_list = [(\"pad\", TFSamepaddingLayer(ksize=7, stride=1))] + module_list\n",
    "\n",
    "        self.conv0 = nn.Sequential(OrderedDict(module_list))\n",
    "        self.d0 = ResidualBlock(64, [1, 3, 1], [64, 64, 256], 3, stride=1)\n",
    "        self.d1 = ResidualBlock(256, [1, 3, 1], [128, 128, 512], 4, stride=2)\n",
    "        self.d2 = ResidualBlock(512, [1, 3, 1], [256, 256, 1024], 6, stride=2)\n",
    "        self.d3 = ResidualBlock(1024, [1, 3, 1], [512, 512, 2048], 3, stride=2)\n",
    "\n",
    "        self.conv_bot = nn.Conv2d(2048, 1024, 1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        def create_decoder_branch(out_ch=2, ksize=5):\n",
    "            module_list = [ \n",
    "                (\"conva\", nn.Conv2d(1024, 256, ksize, stride=1, padding=0, bias=False)),\n",
    "                (\"dense\", DenseBlock(256, [1, ksize], [128, 32], 8, split=4)),\n",
    "                (\"convf\", nn.Conv2d(512, 512, 1, stride=1, padding=0, bias=False),),\n",
    "            ]\n",
    "            u3 = nn.Sequential(OrderedDict(module_list))\n",
    "\n",
    "            module_list = [ \n",
    "                (\"conva\", nn.Conv2d(512, 128, ksize, stride=1, padding=0, bias=False)),\n",
    "                (\"dense\", DenseBlock(128, [1, ksize], [128, 32], 4, split=4)),\n",
    "                (\"convf\", nn.Conv2d(256, 256, 1, stride=1, padding=0, bias=False),),\n",
    "            ]\n",
    "            u2 = nn.Sequential(OrderedDict(module_list))\n",
    "\n",
    "            module_list = [ \n",
    "                (\"conva/pad\", TFSamepaddingLayer(ksize=ksize, stride=1)),\n",
    "                (\"conva\", nn.Conv2d(256, 64, ksize, stride=1, padding=0, bias=False),),\n",
    "            ]\n",
    "            u1 = nn.Sequential(OrderedDict(module_list))\n",
    "\n",
    "            module_list = [ \n",
    "                (\"bn\", nn.BatchNorm2d(64, eps=1e-5)),\n",
    "                (\"relu\", nn.ReLU(inplace=True)),\n",
    "                (\"conv\", nn.Conv2d(64, out_ch, 1, stride=1, padding=0, bias=True),),\n",
    "            ]\n",
    "            u0 = nn.Sequential(OrderedDict(module_list))\n",
    "\n",
    "            decoder = nn.Sequential(\n",
    "                OrderedDict([(\"u3\", u3), (\"u2\", u2), (\"u1\", u1), (\"u0\", u0),])\n",
    "            )\n",
    "            return decoder\n",
    "\n",
    "        ksize = 5 if mode == 'original' else 3\n",
    "        if nr_types is None:\n",
    "            self.decoder = nn.ModuleDict(\n",
    "                OrderedDict(\n",
    "                    [\n",
    "                        (\"np\", create_decoder_branch(ksize=ksize,out_ch=2)),\n",
    "                        (\"hv\", create_decoder_branch(ksize=ksize,out_ch=2)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.decoder = nn.ModuleDict(\n",
    "                OrderedDict(\n",
    "                    [\n",
    "                        (\"tp\", create_decoder_branch(ksize=ksize, out_ch=nr_types)),\n",
    "                        (\"np\", create_decoder_branch(ksize=ksize, out_ch=2)),\n",
    "                        (\"hv\", create_decoder_branch(ksize=ksize, out_ch=2)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.upsample2x = UpSample2x()\n",
    "        # TODO: pytorch still require the channel eventhough its ignored\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        imgs = imgs / 255.0  # to 0-1 range to match XY\n",
    "        d0 = self.conv0(imgs)\n",
    "        d0 = self.d0(d0)\n",
    "        d1 = self.d1(d0)\n",
    "        d2 = self.d2(d1)\n",
    "        d3 = self.d3(d2)\n",
    "        d3 = self.conv_bot(d3)\n",
    "        return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0456d9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_pytorch_checkpoint(net_state_dict):\n",
    "    variable_name_list = list(net_state_dict.keys())\n",
    "    is_in_parallel_mode = all(v.split(\".\")[0] == \"module\" for v in variable_name_list)\n",
    "    if is_in_parallel_mode:\n",
    "        colored_word = colored(\"WARNING\", color=\"red\", attrs=[\"bold\"])\n",
    "        print(\n",
    "            (\n",
    "                \"%s: Detect checkpoint saved in data-parallel mode.\"\n",
    "                \" Converting saved model to single GPU mode.\" % colored_word\n",
    "            ).rjust(80)\n",
    "        )\n",
    "        net_state_dict = {\".\".join(k.split(\".\")[1:]): v for k, v in net_state_dict.items()}\n",
    "    return net_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cf6af39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight_path = 'hover-net-pytorch-weights/hovernet_original_consep_notype_pytorch.tar'\n",
    "weight_path = 'hover-net-pytorch-weights/hovernet_original_kumar_notype_pytorch.tar'\n",
    "net = HoVerNet(input_ch=3, nr_types=None, freeze=True, mode='original')\n",
    "saved_state_dict = torch.load(weight_path, map_location=torch.device('cpu'))[\"desc\"]\n",
    "# saved_state_dict = convert_pytorch_checkpoint(saved_state_dict)\n",
    "net.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b05c8dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "y = net(torch.rand((1,3,1024,1024), dtype=torch.float32))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "835973c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 8, 8])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = nn.Sequential(\n",
    "    nn.Conv2d(2048, 128, 2, 2),\n",
    "    nn.AvgPool2d(2,2),\n",
    "    nn.Conv2d(128, 64, 4, 4),\n",
    "    nn.BatchNorm2d(64)\n",
    ")\n",
    "\n",
    "out(torch.rand((1, 2048, 128, 128), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "243e0c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311232"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_parameters_count(model : nn.Module):\n",
    "    count = 0\n",
    "    for i in list(model.parameters()):\n",
    "        count += list(i.flatten().size())[0]\n",
    "        \n",
    "    return count\n",
    "\n",
    "model_parameters_count(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('torch_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3cdb5011c790a11bb1ea6ed3164cd3b09b9ce057bf32450665e6e9df045eaf43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
